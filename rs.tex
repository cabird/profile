\documentclass[10pt]{article}
% Seems like it does not support 9pt and less. Anyways I should stick to 10pt.
%\documentclass[a4paper, 9pt]{article}
\topmargin-2.0cm

\usepackage{fancyhdr}
\usepackage{pagecounting}
\usepackage[dvips]{color}

% Color Information from - http://www-h.eng.cam.ac.uk/help/tpl/textprocessing/latex_advanced/node13.html

% NEW COMMAND
% marginsize{left}{right}{top}{bottom}:
%\marginsize{3cm}{2cm}{1cm}{1cm}
%\marginsize{0.85in}{0.85in}{0.625in}{0.625in}

\advance\oddsidemargin-1in
%\advance\oddsidemargin-0.65in
%\advance\evensidemargin-1.5cm
\textheight9.2in
\textwidth6.5in
\def\baselinestretch{1.05}
%\pagestyle{empty}

\newcommand{\hsp}{\hspace*{\parindent}}
\definecolor{gray}{rgb}{0.4,0.4,0.4}
%\definecolor{gray}{rgb}{1.0,1.0,1.0}

\newcommand\Section[1]{\subsubsection*{\large #1}}
\newcommand\Subsection[1]{\subsubsection*{\small #1}}

\begin{document}
\thispagestyle{fancy}
%\pagenumbering{gobble}
%\fancyhead[location]{text} 
% Leave Left and Right Header empty.
\lhead{}
\rhead{}
%\rhead{\thepage}
\renewcommand{\headrulewidth}{0pt} 
\renewcommand{\footrulewidth}{0pt} 
%\fancyfoot[C]{\footnotesize \textcolor{gray}{http://wwwcsif.ucdavis.edu/$\sim$bird/index.html}}

%\pagestyle{myheadings}
%\markboth{Sundar Iyer}{Sundar Iyer}

\pagestyle{fancy}
%\lhead{\textcolor{gray}{\it Christian Bird, Research Statement}}
%\rhead{\textcolor{gray}{\thepage/\totalpages{}}}
%\rhead{\thepage}
%\renewcommand{\headrulewidth}{0pt} 
%\renewcommand{\footrulewidth}{0pt} 
%\fancyfoot[C]{\footnotesize http://www.stanford.edu/$\sim$sundaes/application} 
%\ref{TotPages}

% This kind of makes 10pt to 9 pt.
\begin{small}

%\vspace*{0.1cm}
\begin{center}
{\LARGE \bf Research Statement}\\
\vspace*{0.2cm}
{\large Christian Bird}\\
\vspace*{0cm}
\texttt{\normalsize cabird@ucdavis.edu}
\end{center}
%\vspace*{0.2cm}

%\begin{document}
%\centerline {\Large \bf Research Statement for Sundar Iyer}
%\vspace{0.5cm}

% Write about research interests...
%\footnotemark
%\footnotetext{Check This}

\texttt{Software quality is important.  Teams need to produce high quality
software in a timely manner because so much money and other important stuff is
depending on it} As software projects grow in size and complexity, so do the
teams of people that contribute to it.  As this happens, team structure and
process  become critical determinants of programmer productivity, as well as
software quality.  Conway and others have argued that good design and
modularization leads to more effective division of labour and knowledge in
large, complex systems, and thus better quality and productivity outcomes.  My
work has been among the earliest to empirically study the relationships between
software characteristics, team dynamics, and software engineering outcomes in
commercial and open-source software projects.  Long term, my research will
provide empirically grounded  principles that govern the effective co-design of
software and development teams. 


In order to study software development, one needs access to project data such
as communication, source code change activity, issue tracking information,
organizational data, and geographical locations.  Open Source Software (OSS)
projects have proliferated wildly over the past decade.  While the vast
majority of OSS projects either fail to gain a large user and developer base,
there are still a large number of projects whose use, size (in terms of both
developers and source code), complexity, and code quality rival their
commercial counterparts.  These select few such as the Linux kernel, Apache,
Postgres, Python, and Firefox, are worthy of study because they represent
stable codebases developed by teams that deal with many of the same challenges
faced by other large development teams.  As an increasingly prevalent
development style, OSS is important to study, not only in it's own right, but also
in comparison with commercial software.
Fortunately the data and meta-data associated with these projects is publicly
available, providing an exciting opportunity.

I am also interested in the use of rigorous empirical methods to evaluate
questions about software engineering processes.  Over the last few years,
researchers in our field have begun applying more advanced, modern methods
developed in fields such as bio-informatics, econometrics, and organizational
behavior, e.g., statistical network analysis, hazard rate models, and principal
component regression.  More notably, these research trends are beginning to
influence the practice of software development in industry.  My research is at
the heart of these trends.

\Section{Background and Current Work}

How are large software teams oragnized? What factors of team organization
influence productivity and quality?  I have examined the relationships between
software engineers and the code that they develop, contribute, discuss,
release, fix, and maintain, to ask and answer a number of questions: When do
developers need to coordinate their changes?  What are the communication
patterns of the most productive developers?  How does one become a ``core''
project developer?  How do contribution patterns in large projects affect code
quality? Is a team affected by geographic distribution? Does open source really
work by having everyone working on everything?  I have had success in studying
large software projects in order to answer these questions.

While many have studied the globally distributed software development, I
published the first results that examined the effect of distributed development
on post-release failures.  My research on the development of Windows Vista
showed that while there are difficulties inherent in globally distributed
software development, processes such as daily synchronous contact, migration of
expertise, and consistent tools can mitigate their affects.  I found that the
difference in software quality between components that were developed in one
location and those that were spread abroad was nearly all attributable to the
size of the development team rather than distribution level.  This work was
recognized as a SIGSOFT distinguished paper at the top software engineering
conference and was invited to appear as the first software engineering research
paper in the \emph{Research Highlights} section of Communications of the ACM.

One of the problems with any case study is that it is unclear how well the
results generalize and how much they are affected by any number of variables.
I have therefore examined distributed development in the the context of other
projects that use varying development styles.  My results thus far support
the hypothesis that:

When projects have a non-trivial number of distributed components, there is
little difference in quality between distributed and collocated components.
However, when only a few of pieces are distributed, they suffer from a software
process which relies on collocation, leading to many more post release bugs.  A
company cannot hope to successfully move from a collocated development style to
distributed development without modifying processes accordingly.

Socio-technical relationships are strongly related to software quality.  I
built socio-technical networks, composed of developers and software components,
and found that the topological properties of these networks could be used to
predict failure-prone components with precision and recall levels around 90\%.
My results were consistent across Windows Vista and 6 major releases of the
Eclipse project, indicating a result that spans development styles.  In
addition, I devised data transformations that would allow practictioners to use
a model built on a prior release to predict fault-prone modules in unreleased
software.

I've also examined the organizational structure of developers within OSS
projects by adapting clustering methods from the field of complex networks in
physics to identify tight knit teams.  I found that structure is actually much
more similar to commercial contexts than previously thought, with clear teams
of closely collaborating people tackling cohesive tasks. However, we observed
that team formation is more organic and the team makeup is more dynamic than in
commercial settings.

I have observed other key differences between commerical and OSS settings.  In
a study of the effect of ownership on software quality, we found that low
ownership levels of code entities lead to high pre and post-release failures in
commercial-style development.  The effect is much less pronounced in more
open-source style projects; We found that failures are related to team size
much more than the level of ownership by members of teams.  

We used social network status and patch contribution and acceptance data to
answer the question ``What does it take to become a core OSS project
developer?'' We chose projects from diverse domains and governance styles, and
the surprising answer for all of them was that the likelihood is not only
dependent on social status and contributions made, but also has a curvilinear
relationship with time that a participant has spent with the project.
Specifically, members need project-specific expertise and community respect,
both of which increase with time, in addition to commitment, which arguably
wanes over time.  We observed ``peak'' time at which participants become
project developers.  These findings from OSS have been corroborated by
discussions with managers in commercial settings where similar phenomena were
observed.  Due to the need to develop project-specific knowledge and contacts
and also a tendency for an employee's focus and enthusiasm to wane, there is a
peak period of productivity.

We used social network analysis on mined mailing list data from a number of
projects to quantify the behaviour and ``importance'' of OSS project members.
One of the things that we found was a strong link between communication
behavior and development activity for project developers, and that there is
less of a relationship for project documenters, indicating that the need for
communication and coordination in accomplishing tasks is strongly related to
the role that a participant plays.

A research strategy that combines qualitative and quantitative methods can be
highly effective because each compensates for the weaknesses of the other.
This is a common strategy in social science, usually called a ``multi-method''
approach.  I used such an approach when studying an emerging trend in
OSS development, distributed version control, and it's effect on coordination
and process in projects.  As the main channel by which changes to software are
stored and conveyed, the version control system constrains the process that a
software team uses.  I began by interviewing lead developers from OSS projects
that had switched to distributed SCMs in an effort to understand what benefits
they hoped to gain and then performed a quantitive study assessing hypotheses
regarding the effects on developer practices.

I have also explored collaboration in contexts other than software engineering
in an effort to understand what principles are more general, and how these forms of
collaboration differ.  As an example, I studied academic collaboration by
examining patterns of co-authorship of papers in computer science.  One of the
key insights of this work was that disciplines that are more theoretical, such
as theory and cryptography, have communities which tend to collaborate more
freely than disciplines of practice, which include distributed systems and
software engineering, where we observed highly modular collaboration patterns.

\Subsection{Experimental Methods}

Some of these studies of software engineering have required types of data that
has not existed to this point. I have had to develop new techniques for
obtaining and analyzing required data, which have been accepted in the
community and adopted by other researchers.

The preferred method for communication in most OSS projects is via developer
mailing lists.  This allows all people involved to view topics under discussion
and to participate when desired.  As such, the contents of these archived
mailing lists represent the discussion history of a project and are useful for
research.  There are issues in mining such data, however.  Unlike commercial
settings, most contributors use multiple email addresses when participating in
projects, which can lead one to attribute messages to many unique identities
even though they were sent by the same person.  My research introduced this
problem and presented the first mining techniques for overcoming it, which I
used to create and examine the first mined developer communication social
networks.  Attributes of these networks were used (and found to be
statistically significant) in a number of empirical studies.  Since that time,
many other researchers have adopted my method and
this work has received nearly 100 citations.

Volunteer contributions in OSS often come in the form of ``patches'' which are
reified changes to source code.  I introduced a method for both identifying
such contributions and detecting if they were accepted even in the presence of
minor modifications, a common occurrence.  This data was instrumental in
examining the contribution acceptance criteria and requirements needed for
participants to become core project developers.  Again, my techniques have
been adopted by others in their own research and publications.

Distributed version control differs from prior version control systems in many
ways.  Branching and merging is well supported, occurs \textbf{much} more
frequently, and is recorded in the repository.  Data flow between repositories,
authorship information, and audit trails are recorded more precisely as well.
But there are also potential dangers for miners; the history can be revised and
some data may be hidden from view.  We pioneered mining data from distributed
source control and published a guide listing the opportunities as well as
hurdles associated with mining data from distributed SCMs along with solutions
to such problems.  We also used such data to examine how use of distributed version 
control can enable project processes not possible before.

I am also interested in the quality of data that is used in empirical software
engineering.  Corrupted or incomplete data can lead to incorrect results which
can affect both future research and practitioners who may modify their
practices according to them.  To demonstrate this problem, we gathered bug data
from many projects, identified ascertainment bias in the data, and showed how
this bias can lead to false conclusions and biased bug location predictions.
We showed that the bug databases for many OSS projects are incomplete, in that
many bugs that are marked as fixed cannot be accurately ``linked'' to the
correcting change in the project SCM.  We showed that such links are missing in
a way that is heavily biased, e.g. data is missing more often for more severe
bugs than less severe. Further, bug prediction techniques are sensitive
to such problems; when trained on such data, they are more accurate at predicting
less severe bugs.

I believe that it is possible to overcome this problem, however, and am
currently working on ways to combat it.  I have developed tools for efficiently
working with both source code and bug data and have used them with project
developers to recreate missing links which can then be used to determine both
\textbf{why} data was missing in the first place and \textbf{how} to deal with
it's absence after the fact.

\Section{Future Work}

The answer to many software engineering questions is actually ``It depends.''
There is no best software process or best programming language.  Agile and
scrum work quite well for some projects, but not for others.  My research
focuses on the follow-up question, ``Yes, it depends, but on what?'' I have
found, for instance, that when certain processes are used, software quality
\textbf{does not} depend on geographic distribution.  I \textbf{have} found
that in both open source and industrial contexts, patterns of socio-technical
relationships are indicative of failure prone software components.  I plan to
investigate what attributes of software projects affect coordination needs and
the relationship of social structure with outcomes.

\Subsection{Studying Diverse Domains}

Games and web applications are growing in importance, and are attracting
enormous investments in terms of time, money, and talent. However, there are
still as yet very few studies of development projects in these areas. Are these
domains like traditional software engineering? If not, how are they different?
The answers to these questions help us determine what principles are more
universal and which are specific to characteristics of software projects.  This
can, in turn, aid plannng and governance of software projects by informing
stakeholders of how decisions may affect productivity and quality, and ultimately
timelines and cost.

The video game industry represents \$10's of billions annually. However, game
development differs from the development of typical products in some important
ways.  Are the principles of software engineering presented in prior research
affected by these differences?

\begin{enumerate}

\item Content - Most games are split into an engine, which represents executable code,
    and data content. Compared to other software domains, a disproportionately large
    amount of money, time, talent, and space is devoted on content.  
    Many software engineering research techniques, such as
    bug prediction techniques, awareness tools, and recommender systems, use attributes
    of source code. Thus, these techniques are of limited value
    in a game development context where many of the tasks and contributors are
    not related directly to the code. I plan to explore the relationship between content
    and code relationships in an effort to devise new
    methods for bug prediction, team member awareness, and expert recommendation.

\item Team Makeup - A sizeable proportion of team members
    are not software developers, including artists, story line, and other content creators.
    One might well expect that the communication and coordination needs of members
    in content creation and content testing roles differ from software developers.  I plan to observe, record, and mine
    interactions, socio-technical relationships, and information 
    needs in order to characterize the differences in these roles
    and correlate them with outcomes, leading 
    to an increased understanding of the dynamics in game development teams.

\item Closed Systems - In the past most gaming software has run in a relatively closed
    system. Console games run on a very restricted set of hardware and the set of ``installed''
    software is constrained.  However, this is changing as games are moving online,
    must support downloadable content, and are supporting more complex and varied input 
    mechanisms.  As games move to a more open and less predictable world,
    how do these differences affect the testing needed to assure quality and stability?  

\end{enumerate}

Web applications are becoming more and more complex and prevalent.  The vast
majority of everyday computing tasks can be accomplished via any notebook
equipped with a web browser.  The architecture of these applications often spans a number of
machines from the client to a web server to a backend data store.  Development of these
applications has many differences from traditional development.

\begin{enumerate}

\item Live Hosting - 
    In stark contrast to traditional development where a relatively static
    product is downloaded and installed on a user's machine, web applications are repeatedly ``deployed''
    whenever they are accessed by a browser.  Many web applications 
    claim to update production code multiple times per week and 
    test new features by observing the effects of enabling them for a subset of their users. 
    This gives development teams greater flexibility in terms of timelines and release cycles, but
    also creates great risk, as a bug on one server can (and has) make an application unusable
    for the entire user-base.
    I plan to investigate the opportunities and costs associated with this key difference.

\item Monitoring - Since web applications are hosted on servers, usage of the software
    can be monitored much more closely than traditional applications.  While companies have used limited
    user studies, opt-in data gathering systems, and crash reporting applications, it is difficult
    to determine how an application is being used ``in the field.'' For instance, crash reporting
    can provide the context of a program failure, but gives no indication of what parts of an application
    are used heavily and \textbf{not} failing, or even what proportion of users are experiencing problems.
    The ability to see what parts of an application are being used most, where
    performance degrades, and how features are misused has many benefits.  
    This data can be used to direct resource allocation and for
    feature planning.  It can also provide rich information about failures such as what proportion of users 
    are being affected and what the contextual differences between failures and non-failures are.

\end{enumerate}


\Subsection{Dealing with Data Quality Issues}

I also plan to investigate methods of dealing with data quality issues.  My
research has succeeded in showing the ways in which poor data can lead to
incorrect or biased results.  The next steps include devising methods for
increasing the quality of data and finding techniques that are as robust to
noisy data as possible.

Part of dealing with poor data comes in determining why the data is not complete
in the first place.  Do developers intentionally leave out linking data when it would
implicate them as the person that originally introduced the bug?  Are there times
in a release cycle when developers are more diligent about keeping records?  Understanding
\textbf{why} data is missing can help in both recovering it and altering existing technques
for analyzing it.

\vspace{0.5cm}

\end{small}

\begin{footnotesize}

%\bibliographystyle{IEEEtran}
%\bibliography{bird}

\end{footnotesize}

\end{document}

