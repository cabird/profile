\documentclass[10pt]{article}
% Seems like it does not support 9pt and less. Anyways I should stick to 10pt.
%\documentclass[a4paper, 9pt]{article}
\topmargin-2.0cm

\usepackage{fancyhdr}
\usepackage{pagecounting}
\usepackage[dvips]{color}

% Color Information from - http://www-h.eng.cam.ac.uk/help/tpl/textprocessing/latex_advanced/node13.html

% NEW COMMAND
% marginsize{left}{right}{top}{bottom}:
%\marginsize{3cm}{2cm}{1cm}{1cm}
%\marginsize{0.85in}{0.85in}{0.625in}{0.625in}

\advance\oddsidemargin-1in
%\advance\oddsidemargin-0.65in
%\advance\evensidemargin-1.5cm
\textheight9.2in
\textwidth6.5in
\def\baselinestretch{1.05}
%\pagestyle{empty}

\newcommand{\hsp}{\hspace*{\parindent}}
\definecolor{gray}{rgb}{0.4,0.4,0.4}
%\definecolor{gray}{rgb}{1.0,1.0,1.0}

\newcommand\Section[1]{\section*{#1}}
\newcommand\Subsection[1]{\subsubsection*{#1}}

\begin{document}
\thispagestyle{fancy}
%\pagenumbering{gobble}
%\fancyhead[location]{text} 
% Leave Left and Right Header empty.
\lhead{}
\rhead{}
%\rhead{\thepage}
\renewcommand{\headrulewidth}{0pt} 
\renewcommand{\footrulewidth}{0pt} 
\fancyfoot[C]{\footnotesize \textcolor{gray}{http://wwwcsif.ucdavis.edu/$\sim$bird/index.html}}

%\pagestyle{myheadings}
%\markboth{Sundar Iyer}{Sundar Iyer}

\pagestyle{fancy}
\lhead{\textcolor{gray}{\it Christian Bird, Research Statement}}
\rhead{\textcolor{gray}{\thepage/\totalpages{}}}
%\rhead{\thepage}
%\renewcommand{\headrulewidth}{0pt} 
%\renewcommand{\footrulewidth}{0pt} 
%\fancyfoot[C]{\footnotesize http://www.stanford.edu/$\sim$sundaes/application} 
%\ref{TotPages}

% This kind of makes 10pt to 9 pt.
\begin{small}

%\vspace*{0.1cm}
\begin{center}
{\LARGE \bf RESEARCH STATEMENT}\\
\vspace*{0.1cm}
{\normalsize Christian Bird (cabird@ucdavis.edu)}
\end{center}
%\vspace*{0.2cm}

%\begin{document}
%\centerline {\Large \bf Research Statement for Sundar Iyer}
%\vspace{0.5cm}

% Write about research interests...
%\footnotemark
%\footnotetext{Check This}

As software projects grow in terms of size and complexity, so do the teams of
people that contribute to it in any of a myriad of ways.  As this phenomenon
becomes more pronounced, the social aspects of software development become more
prominent and play important roles.  However, as Conway proposed in 1968, I
believe that the social aspects of software engineering are intimately tied
with the technical artifact itself.  My research interests lie largely in the
relationship between characteristics of software (e.g. architecture, language,
process used, domain) and the coordination and communication in the
teams that develop and maintain it.

I am also interested in the use of rigorous empirical methods to evaluate
questions about software engineering processes.  Unfortunately, the state
of quantitative analysis in software engineering lags tremendously behind
other fields such as econometrics, organizational behavior, and sociology.
In the best case, misapplied empirical methods leave research questions
unanswered, while in the worst case they lead to wrong answers which are believed
to be empirically correct upon which further research may rest.  Software
companies change their practices and perform self-analysis based on results
of empirical research, thus results that are based on incorrect data or even
over-generalizes results has a very real effect on both researchers and 
practitioners.

\Section{Background and Current Work}

In order to study software development, one needs access to project data such
as communication, source code change activity, issue tracking information,
organizational data, and geographical locations.  While I have had success
using commercial data, it is difficult and time consuming to
obtain such information from commercial entities. 

Open Source Software (OSS) projects have proliferated wildly over the past
decade.  While the vast majority of OSS projects either fail, or are unable (or
are unintended) to gain a large user and developer base, there are still a
large number of projects whose use, size --- in terms of both developers and source
code ---, complexity, and code quality rival their commercial counterparts.  These
select few (though they do still number in the tens to hundreds) such as the
Linux kernel, Apache, Postgres, Python, and Firefox, are worthy of study
because they represent stable codebases developed by large developer teams.
These teams deal with nearly all of the challenges faced by other large
development teams: coordination of many developers, changing requirements,
complaints fram users, bugs, maintenance issues and code rot,
constraining architectural decisions, etc.  Fortunately the data and meta-data
associated with these projects is publicly available, providing an exciting
opportunity for software engineering researchers.

\Subsection{Mining Methods}

My research has focused on techniques for mining data from software projects
and then analyzing this data to answer questions about software engineering.  I
have pioneered a number of mining techniques which have been successful in
obtaining data needed for my analysis and also adopted by others in their own
work.

The preferred method for communication in most OSS projects is via developer
mailing lists.  This allows all people involved to view topics under discussion
and to participate when desired.  It also lets project members return to a
discussion to determine who made project decisions and why they were made.  As
such, the contents of these archived mailing lists represent the discussion
history of a project and are useful for research.

There are issues in mining such data, however.  Unlike commercial settings, most
contributors use multiple email addresses when participating in projects, which
can lead one to attribute messages to many unique identities even though they were
sent by the same person.  My research introduced this problem and presented the
first mining techniques for dealing with this email aliasing issue and 
related issues in linking mailing list personalities with accounts source code
management (SCM) systems.  I used this to create and examine of the
first mined developer social networks.  Attributes of these networks were
used (and found to be statistically significant) in a number of empirical studies
later in my career.

As most OSS projects rely on volunteer labor, the process by which work gifts
are accepted by the project is a critical aspect of a project.  Volunteer
contributions often come in the form of ``patches'' which are reified changes
to source code that can be posted to a mailing list or otherwise delivered to
the inner circle of developers who have write access to the project repository.  I
introduced a method for both identifying such contributions and
detecting if they were accepted into the official code base for the project
even if minor modifications were made to the patch (which they often are).  This
data was instrumental in examining the requirements needed for participants
to become code project developers.

I have also studied an emerging trend in OSS development, distributed version
control, and it's effect on coordination and process in projects.  As the main
channel by which changes to software are stored and conveyed, the version
control system constrains the process that a software team uses.  We pioneered
mining data from distributed source control and published a guide listing the
opportunities as well as hurdles associated with mining data from distributed
SCM's along with solutions to such problems.


\Subsection{Communication and Coordination}

The mining of data is only a means to an end.  The goal of such
effort has always been to use the gathered data to learn about how software
projects work.  To that end, I have performed a number of empirical studies
to answer questions about the relationship of social aspects of projects
with the software itself.

We used social network analysis on mined mailing list data from a number of projects 
to quantify the behaviour and ``importance'' of OSS project members.
We found that there is a strong link between communication behavior and
development activity for project developers, and that there is less of a
relationship for project documenters.  Thus, the need for communication and
coordination to accomplish one's task is strongly affected by the role that a
participant plays in a project.  This has strong managerial implications; it may
be beneficial for project leaders to coordinate developers differently than
other contributors.

We used both social network data and patch contribution and acceptance data in
conjunction with data gathered previously to answer the question ``What does it
take to gain admittance to the inner circle of committing developers?'' The
surprising answer for all of the projects studied, which we deliberately chose
to cover diverse domains and governance styles, was that the likelihood is not
only dependent on social status and contributions made, but also
non-monotonically linked with time that a participant has spent with the
project.  Specifically, members need project-specific expertise as well as
community respect, something that increases with time, in order to become a
full fledged developer.  In contrast, commitment, which arguably wanes over
time, is also needed.  We observed a ``peak'' time at which participants become
project developers.  While these findings represent OSS development, there are
clear analogs and implications for other styles of development.

From my experience doing research, I have found that relying on data in a
purely quantitative fashion can lead to wrong conclusions or at best, hides
opportunities to understand it better.  We therefore used a quantitative
approach to determine why so many OSS projects were switching from a
centralized SCM system to a distributed one.  By interviewing developers from a
number of projects and determining the differences in development activity
prior to and following their switch, we were able to identify the benefits that
projects enjoyed, or hope to enjoy by such a move.  Many of these benefits came
down to affecting how developers were able to coordinate their activities and
share their changes.

While many have studied the globally distributed software development, I
published the first results that examined the effect of distributed development
on post-release failures, an important characteristics of
software.  My research on the development of Windows Vista showed that while
there are difficulties inherent in globally distributed software development,
such as cultural differences and lack of easy synchronous communicating, 
certain processes can mitigate their affects.  I found that
the difference in software quality between components that were developed in one
location and those that were spread abroad was nearly all attributable to the
size of the development team.  Put simply, a component that is developed by twenty
people in one buildng and twenty people scattered across the globe have the
same number of post-release failures.

One of the problems with any case study (and one often overlooked by the
authors of such a work) is that it is unclear how well the results generalize
and how much they are affected by any number of variables.  In an effort to
understand distributed development better, I have also examined it in the context 
of other projects that use varying processes; some open source, and some which
represent a hybrid process which incorporates open source style development.
The results of my analysis have led to the following conclusion:

When projects have a non-trivial number of distributed components, there is
little difference in quality between distributed and collocated components.
However, when only a few of the components are distributed, such components
suffer from a software process in which the effects of distributed development
are not taken into account, leading to many more post release bugs.  A
company cannot move from a collocated development
style to distributed development without modifying processes accordingly.
However, the move \textbf{can} be made with little effect on code quality.

I have also explored coordination in contexts other than software engineering.
As an example, I studied academic collaboration by examining patterns of
co-authorship of papers in computer science.  One of the key insights of this
work was that disciplines that are more theoretical, such as theory and
cryptography, have communities which tend to collaborate more freely than
disciplines of practice, which include distributed systems and software
engineering, where we observed highly modular collaboration patterns.  However,
theoretical disciplines also do not draw researchers that are as
multidisciplinary as others, such as the data mining community, which sees
work from researchers who are experts in a variety of other fields


\Subsection{Effects of Data Quality}

I am also interested in the quality of data that is used in empirical software
engineering.  Corrupted or incomplete data can lead to incorrect results which
can affect both future research and practitioners who may modify their
practices according to them.  To demonstrate this problem, we gathered bug data
from many projects, identified bias in the data, and showed how this bias can
lead to false conclusions and biased bug location predictions.

BugCache is an award winning bug prediction technique based on tracking the
locality of bugs and their fixes.  We showed that the bug databases for many
OSS projects are incomplete, in that many bugs that are marked as fixed cannot
be accurately ``linked'' to the correcting change in the project SCM.  Through
the use of rigorous statistical methods we showed that such links are missing
in a way that is heavily biased in almost all projects and that when used on
such biased data, BugCache only predicts similarly biased bugs.  One vitally
important dimensions of bias in bug data is severity.  More severe bugs are
less likely to be linked to their changes.  Thus, a bug prediction method will
tend to predict less severe bugs much more accurately than more severe ones,
which is likely the exact opposite of what a practitioner would use it for.

I believe that it is possible to overcome this problem, however, and am
currently working on ways to combat it.  I have developed tools for efficiently
working with both source code and bug data and have used them with project
developers to recreate missing links which can then be used to determine both
\textbf{why} data was missing in the first place and \textbf{how} to deal with
it's absence after the fact.

\Section{Social Empirical Software Engineering -- A Research Agenda}

I believe that the answer to many software engineering questions is in fact
``It depends.'' There is no best software process or best programming
language.  Agile and scrum work quite well for some projects, but won't for all
of them.  Sometimes adding people to software project actually \textbf{is} the
best decision.  Thus, searching for universal answers to such questions may be
misleading.  There is real value in determining what the answers to questions
actually depend on.  I have found, for instance, that when certain processes
are used, software quality \textbf{does not} depend on geographic distribution.
I \textbf{have} found that in both open source and industrial contexts,
patterns of socio-technical relationships are indicative of failure prone
software components.  I plan to investigate what attributes of software
projects affect coordination needs and the effects of social structure and in what
ways. 

\Subsection{Studying Diverse Domains}

Game development and web applications are areas of software engineering that
remain underrepresented in research.  Therefore, there is little reason to believe
that prior results hold in such settings.  Yet both of these areas are growing
segments of the software industry in terms of money, talent, and market share.
Studies of these domains have benefit in two primary ways.  Identifying the
differences in these domains relative to other development fields help to
understand what software engineering principles and results are unlikely to
hold.  In contrast, when results from prior studies are supported
in these domains, this is evidence that such results are universal and broadly
applicable, a major contribution to the body of software engineering knowledge.

The video game industry represents \$10's of billions annually. Indeed
companies such as Microsoft, Sony, Nintendo, and EA spend hundreds of
millions of dollars each year developing gaming software for computers and
consoles.  However, game development differs from the development of typical
products in some important ways.

\begin{enumerate}

\item Content - Most games can be split into an engine, which represents executable code,
    and data content.  Compared to other software domains, a disproportionately large
    amount of money, time, talent, and space on content.  Yet there are surely problems,
    "bugs", that occur in the content.  Much of software engineering research, such as
    bug prediction techniques, awareness tools, and recommender systems, use attributes
    of and relationships in source code.  Thus, these techniques are of limited value
    in a game development context.  I plan to explore the relationship between content
    and engine as well as other intra-content relationships in an effort to devise new
    methods for bug prediction, team member awareness, and expert recommendation.

\item Team Makeup - Due to the large amount of content, a sizeable proportion of team members
    are not software developers, but rather story-writers, graphic artists, sound engineers,
    etc.  One might well expect that the communication and coordination needs of members
    in such roles differs from software developers in the project.  It may be that similar
    to the difference between documentation and development, there is less of a coordination
    requirement for non-developers, or these members may simply require different communication
    patterns to complete their tasks.  I plan to observe, record, and mine
    interactions and information needs in order to characterize the differences in these roles
    and correlate them with outcomes such as software bugs and release delays.  This will lead
    to an increased understanding of the dynamics in game development teams.

\item Closed Systems - Though slowly changing, most gaming software runs in a relatively closed
    system.  Console games run on a very restricted set of hardware.  The set of ``installed''
    software is also constrained.  Except for portions of the console operating systems, 
    components within a game software stack are unlikely to be upgraded on the fly, a problem which
    causes library dependency headaches and other problems on normal computers.  
    Relative to other applications that are data directed (Database Servers, Web Browsers, 
    Office Applications), the set of data for games is either known in advance or,
    in the case of downloadable content, screened and and modified if needed prior to deployment.
    How do these differences affect the testing needed to assure quality and stability?  

\end{enumerate}

Web applications are becoming more and more complex and prevalent.  The vast
majority
of everyday computing tasks can be accomplished via any notebook
equipped with a web browser.  Javascript (the primary language of web
applications) is now one of the the world's most used and executed programming languages.  The architecture of these applications often spans a number of machines from the client to a web server to a back store.  Development of these applications has many differences from
traditional development.
I list here just a few.

\begin{enumerate}

\item Live Hosting - If any notion of ``deployment to the user'' exists in the context of web applications,
    is occurs when a user access a site and the data and client side code is downloaded to the 
    browser.  This is in stark contrast to traditional development where a relatively static
    product is downloaded and installed on a user's machine.  If changes need to be made to a web
    application, there is no need to ship out a new release to users or even supply a patch or hotfix
    via some update mechanism.  Accordingly, the excessive cost of such deployment mechanisms is also
    absent (though deployment and hosting of web applications is hardly a free enterprise).  This gives
    development teams greater flexibility in terms of timelines, and release cycles.  Teams may decide
    to ship new features as they become available rather than hit a coordinated release.  Bugs can be fixed
    much more quickly.  Google has claimed that it actually tests new features by sending a fraction of it's
    users to a newer version of an application and examining the results.  I plan to investigate
    what opportunities this important difference affords web application teams in terms of the processes
    used.

\item Monitoring - Since web applications are hosted on servers owned by the company, usage of the software
    can be monitored much more closely than traditional applications.  While companies have used limited
    user studies, opt-in data gathering systems, and crash reporting applications, it has been difficult
    to determine how an application is being used ``in the field.'' Much as Watson and other crash
    reporting software has changed how development decisions are made with Microsoft and other companies,
    the ability to see what parts of an application are being used, what is causing crashes, and where
    performance degrades is valuable.  I believe that this data can be used to direct resource allocation and for
    feature planning.  It can also indicate which failures are affect the most users and give much
    more information about the failures.

\end{enumerate}

Gaining an understanding of the differences and similarities of these domains with other, well
represented, software development domains is important because 

\Subsection{Dealing with Data Quality Issues}

I also plan to investigate methods of dealing with data quality issues.  My
research has succeeded in showing the ways in which poor data can lead to
incorrect or biased results.  The next steps include devising methods for
increasing the quality of data and finding techniques that are as robust to
noisy data as possible.

Part of dealing with poor data comes in determining why the data is not complete
in the first place.  Do developers intentionally leave out linking data when it would
implicate them as the person that originally introduced the bug?  Are there times
in a release cycle when developers are more diligent about keeping records?  Understanding
\textbf{why} data is missing can help in both recovering it and altering existing technques
for analyzing it.

\vspace{0.5cm}
%\begin{flushright}
%Sundar Iyer
%\end{flushright}

\end{small}
%\newpage

\begin{footnotesize}

%\bibliographystyle{IEEEtran}
%\bibliography{bird}

\end{footnotesize}

\end{document}

